{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c7240cc0",
   "metadata": {},
   "source": [
    "# calling libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e51ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import random\n",
    "import tensorflow.keras as keras\n",
    "# from keras import backend as K\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "import pywt\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from scipy.signal import butter, lfilter\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b511d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_SIZE = 8"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1b48ef0d",
   "metadata": {},
   "source": [
    "# main functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa361b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is for getting the same results.\n",
    "def reset_random_seeds(seed):\n",
    "   os.environ['PYTHONHASHSEED']=str(seed)\n",
    "   tf.random.set_seed(seed)\n",
    "   np.random.seed(seed)\n",
    "   random.seed(seed)\n",
    "reset_random_seeds(42)\n",
    "\n",
    "def successive(successive):\n",
    "    input_data=[]\n",
    "    for i in range(2 * (INPUT_SIZE-1), len(successive)):\n",
    "        if INPUT_SIZE == 4:\n",
    "            input_data.append([successive[i-3]]+[successive[i-2]]+[successive[i-1]]+[successive[i]])\n",
    "        else:\n",
    "            input_data.append([successive[i-7]]+[successive[i-6]]+[successive[i-5]]+[successive[i-4]]+[successive[i-3]]+[successive[i-2]]+[successive[i-1]]+[successive[i]])\n",
    "    return input_data  \n",
    "\n",
    "#wavelet transform\n",
    "def dwt(training):\n",
    "    input_data=np.array(training)\n",
    "    days = input_data[:,0:INPUT_SIZE]\n",
    "    (a, d) = pywt.dwt(days, 'haar')\n",
    "    (a2,d2)=pywt.dwt(a, 'haar')\n",
    "    if INPUT_SIZE == 4:\n",
    "        l3=np.append(a2,d2, axis=1)\n",
    "        l2_3=np.append(l3,d, axis=1)\n",
    "        transformed_df=l2_3        \n",
    "    else:\n",
    "        (a3,d3)=pywt.dwt(a2, 'haar')\n",
    "        l3=np.append(a3,d3, axis=1)\n",
    "        l2_3=np.append(l3,d2, axis=1)\n",
    "        l3_3=np.append(l2_3,d, axis=1)\n",
    "        transformed_df=l3_3     \n",
    "\n",
    "    training=transformed_df[:,1:]\n",
    "    return training\n",
    "\n",
    "def idwt(transformed_data):\n",
    "    (a, d) = np.hsplit(transformed_data, 2)\n",
    "    (a2, d2) = np.hsplit(a, 2)\n",
    "    if INPUT_SIZE == 4:\n",
    "        idwt_res = pywt.idwt(a2, d2, 'haar')\n",
    "        reconstructed_df = pywt.idwt(idwt_res, d, 'haar')\n",
    "    else:\n",
    "        (a3, d3) = np.hsplit(a2, 2)\n",
    "        idwt_res = pywt.idwt(a3, d3, 'haar')\n",
    "        idwt_res2 = pywt.idwt(idwt_res, d2, 'haar')\n",
    "        reconstructed_df = pywt.idwt(idwt_res2, d, 'haar')\n",
    "    \n",
    "    return reconstructed_df\n",
    "\n",
    "def get_data(path, type, start, stop):\n",
    "    input_data = pd.read_csv(path)\n",
    "    input_data = np.array(input_data)[start:stop][:,type]\n",
    "    input_data = np.array(input_data)\n",
    "    input_data = input_data.reshape(input_data.shape[0])\n",
    "    input_data = list(input_data)\n",
    "    \n",
    "    return input_data\n",
    "\n",
    "def preprocess_data(data_list):\n",
    "    if len(data_list) == 1:\n",
    "        input_data = data_list[0]\n",
    "        shifted_input_data = input_data[24-INPUT_SIZE:]\n",
    "        input_data = np.array(input_data[:-24])\n",
    "        out_data = shifted_input_data[INPUT_SIZE:]\n",
    "        shifted_input_data = np.array(shifted_input_data[:-INPUT_SIZE])\n",
    "        input_data_successive = successive(input_data)\n",
    "        second_data_successive = successive(shifted_input_data)\n",
    "        out_data_successive = successive(out_data)\n",
    "    else:\n",
    "        input_data = np.array(data_list[0])\n",
    "        shifted_input_data = np.array(data_list[1])\n",
    "        out_data = np.array(data_list[2])\n",
    "        input_data_successive = successive(input_data)\n",
    "        second_data_successive = successive(shifted_input_data)\n",
    "        out_data_successive = successive(out_data)\n",
    "    return input_data, out_data_successive, input_data_successive, second_data_successive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0be210",
   "metadata": {},
   "outputs": [],
   "source": [
    "#network configurations\n",
    "hidden1=16\n",
    "second_layer1=16\n",
    "third_layer1=16\n",
    "forth_layer1=8\n",
    "\n",
    "hidden2=16\n",
    "second_layer2=16\n",
    "third_layer2=16\n",
    "forth_layer2=8\n",
    "\n",
    "hidden3=16\n",
    "second_layer3=16\n",
    "third_layer3=16\n",
    "forth_layer3=8\n",
    "\n",
    "hidden4=16\n",
    "second_layer4=16\n",
    "third_layer4=16\n",
    "forth_layer4=8\n",
    "\n",
    "hidden5=16\n",
    "second_layer5=16\n",
    "third_layer5=16\n",
    "forth_layer5=8"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cfe32df0",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b173e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = []\n",
    "data_list.append(get_data('original_code/data/hourly/63.11.HG.csv', type=5, start=12077, stop=13346))\n",
    "data_list.append(get_data('original_code/data/hourly/63.13.HG.csv', type=9, start=11977, stop=13246))\n",
    "data_list.append(get_data('original_code/data/hourly/63.12.HG.csv', type=13, start=12065, stop=13334))\n",
    "input_data, out_data_successive, input_data_successive, second_data_successive = preprocess_data(data_list)\n",
    "# type\n",
    "# 5: Temperature\n",
    "# 9: Relative Humidity\n",
    "# 13: Wind Speed\n",
    "\n",
    "#division of data set into training and test data set\n",
    "N = len(input_data)\n",
    "division_of_training = 0.9\n",
    "input_train = input_data_successive[:int(N*division_of_training)]\n",
    "input_test = input_data_successive[int(N*division_of_training):int(N)]\n",
    "\n",
    "input_train_today = second_data_successive[:int(N*division_of_training)]\n",
    "input_test_today = second_data_successive[int(N*division_of_training):int(N)]\n",
    "\n",
    "output_train = out_data_successive[:int(N*division_of_training)]\n",
    "output_test = out_data_successive[int(N*division_of_training):int(N)]\n",
    "\n",
    "#normalization\n",
    "\n",
    "subtraction_first_train = np.mean(np.array(input_train), axis=1)\n",
    "subtraction_first_test = np.mean(np.array(input_test), axis=1)\n",
    "\n",
    "subtraction_second_train = np.mean(np.array(input_train_today), axis=1)\n",
    "subtraction_second_test = np.mean(np.array(input_test_today), axis=1)\n",
    "\n",
    "subtraction_output_train = np.mean(np.array(output_train), axis=1)\n",
    "subtraction_output_test = np.mean(np.array(output_test), axis=1)\n",
    "\n",
    "#normalization of inputs\n",
    "\n",
    "first_input_train=input_train-subtraction_first_train[:, None]\n",
    "first_input_test=input_test-subtraction_first_test[:, None]\n",
    "\n",
    "output_train_normalized=output_train-subtraction_output_train[:, None]\n",
    "output_test_normalized=output_test-subtraction_output_test[:, None]\n",
    "\n",
    "second_input_train=input_train_today-subtraction_second_train[:, None]\n",
    "second_input_test=input_test_today-subtraction_second_test[:, None]\n",
    "\n",
    "#4inputs WT\n",
    "final_first_w_input_train=dwt(first_input_train)\n",
    "final_first_w_input_test=dwt(first_input_test)\n",
    "output_w_train = dwt(output_train_normalized)\n",
    "output_w_test = dwt(output_test_normalized)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fb2a75b8",
   "metadata": {},
   "source": [
    "## PECNET function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642641b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PECNET(X_train, y_train, X_test, y_test):\n",
    "    X_train=np.array(X_train)\n",
    "    y_train=np.array(y_train)\n",
    "\n",
    "    X_test=np.array(X_test)\n",
    "    y_test=np.array(y_test)\n",
    "\n",
    "    m_primary=len(X_train[0,:])\n",
    "    p_primary=np.size(y_train[0])\n",
    "    N_primary=len(X_train)\n",
    "\n",
    "    model= Sequential ([\n",
    "        Dense(hidden1, input_dim=m_primary, activation='relu'), \n",
    "        Dropout(0.1),\n",
    "        Dense(second_layer1), #,activation='relu'),\n",
    "        Dropout(0.1),\n",
    "        Dense(third_layer1), #,activation='relu'),\n",
    "        Dropout(0.1),\n",
    "        Dense(forth_layer1), #,activation='relu'),\n",
    "        Dropout(0.1),\n",
    "        Dense(p_primary)\n",
    "        ])\n",
    "        \n",
    "    #model.summary()\n",
    "\n",
    "    #sgd=keras.optimizers.legacy.SGD(lr=0.05,momentum=0.75, decay=0.0, nesterov=False)\n",
    "    rmsprop = keras.optimizers.legacy.RMSprop(lr=0.05,momentum=0.75, decay=0.0)\n",
    "    adam = keras.optimizers.legacy.Adam(learning_rate=0.01)\n",
    "    model.compile(loss='mean_squared_error', optimizer=adam, metrics=['mean_absolute_error','mean_squared_logarithmic_error','cosine_similarity','logcosh'])\n",
    "    history1=model.fit(X_train, y_train, batch_size=int(N_primary/10), epochs=300, shuffle=False, verbose=0)  \n",
    "\n",
    "    predicted_train = model.predict(X_train) \n",
    "    predicted_train = np.reshape(predicted_train, (predicted_train.size,))\n",
    "    error_train1=predicted_train-y_train\n",
    "\n",
    "    predicted_test = model.predict(X_test) \n",
    "    predicted_test = np.reshape(predicted_test, (predicted_test.size,))\n",
    "    error_test1=predicted_test-y_test\n",
    "\n",
    "    error_train=pd.DataFrame(error_train1)\n",
    "    add_train=dwt(second_input_train) \n",
    "    \n",
    "    X_error_train1=np.array(add_train)\n",
    "    y_error_train1=np.array(error_train)\n",
    "\n",
    "    error_test=pd.DataFrame(error_test1)\n",
    "    add_test=dwt(second_input_test) \n",
    "\n",
    "    X_error_test1=np.array(add_test)\n",
    "\n",
    "    m_second=len(X_error_train1[0,:])\n",
    "    p_second=np.size(y_train[0])\n",
    "    N_second=len(X_error_train1)\n",
    "\n",
    "    error_model1= Sequential ([\n",
    "        Dense(hidden2, input_dim=m_second, activation='relu'), \n",
    "        Dropout(0.1),\n",
    "        Dense(second_layer2), #,activation='relu'),\n",
    "        Dropout(0.1),\n",
    "        Dense(third_layer2), #,activation='relu'),\n",
    "        Dropout(0.1),\n",
    "        Dense(forth_layer2), #,activation='relu'),\n",
    "        Dropout(0.1),\n",
    "        Dense(p_second)\n",
    "    ])\n",
    "\n",
    "    #error_model1.summary()\n",
    "\n",
    "    #sgd=keras.optimizers.legacy.SGD(learning_rate=0.05, momentum=0.75, decay=0.0, nesterov=False)\n",
    "    rmsprop = keras.optimizers.legacy.RMSprop(lr=0.05,momentum=0.75, decay=0.0)\n",
    "    error_model1.compile(loss='mean_squared_error', optimizer=adam, metrics=['mse','mae','accuracy'])\n",
    "    history3=error_model1.fit(X_error_train1, y_error_train1, batch_size=N_second, epochs=300, shuffle=False, verbose=0)\n",
    "\n",
    "    error_predicted_tr = error_model1.predict(X_error_train1)\n",
    "    error_predicted_tr = np.reshape(error_predicted_tr, (error_predicted_tr.size,))\n",
    "    error_predicted_tes = error_model1.predict(X_error_test1)\n",
    "    error_predicted_tes = np.reshape(error_predicted_tes, (error_predicted_tes.size,))\n",
    "\n",
    "    compensated1_train=(predicted_train+subtraction_second_train)-(error_predicted_tr)\n",
    "    compensated1_test=(predicted_test+subtraction_second_test)-(error_predicted_tes)\n",
    "\n",
    "    error_train2a=compensated1_train-(y_train+subtraction_second_train)\n",
    "    error_test2a=compensated1_test-(y_test+subtraction_second_test)\n",
    "\n",
    "    error_train2=pd.DataFrame(error_train2a)\n",
    "    for i in range(INPUT_SIZE):\n",
    "        error_train2[i+1]= error_train2[i].shift(1)\n",
    "    error_train2 = error_train2.replace(np.nan, 0)\n",
    "    error_train2 = error_train2.iloc[:,::-1]\n",
    "    ##error normalization\n",
    "    subtraction_error_train2=np.mean(np.array(error_train2)[:,:-1], axis=1)\n",
    "    error_train2=error_train2-subtraction_error_train2[:, None]\n",
    "    error_train2=np.array(error_train2)\n",
    "    days_train = error_train2[:,:INPUT_SIZE]\n",
    "    input3_train=dwt(days_train)\n",
    "    output3_train=error_train2[:,INPUT_SIZE]\n",
    "\n",
    "    X_error_train2=np.array(input3_train)\n",
    "    y_error_train2=np.array(output3_train)\n",
    "\n",
    "    error_test2=pd.DataFrame(error_test2a)\n",
    "    for i in range(INPUT_SIZE):\n",
    "        error_test2[i+1]= error_test2[i].shift(1)\n",
    "    error_test2 = error_test2.replace(np.nan, 0)\n",
    "    error_test2 = error_test2.iloc[:,::-1]\n",
    "\n",
    "    subtraction_error_test2=np.array(error_test2)\n",
    "    subtraction_error_test2=subtraction_error_test2[:,:-1]\n",
    "    subtraction_error_test2=np.mean(subtraction_error_test2, axis=1)\n",
    "    error_test2=error_test2-subtraction_error_test2[:, None]\n",
    "\n",
    "    error_test2=np.array(error_test2)\n",
    "    days_test = error_test2[:,:INPUT_SIZE]\n",
    "    input3_test=dwt(days_test)\n",
    "    output3_test=error_test2[:,INPUT_SIZE]\n",
    "\n",
    "    X_error_test2=np.array(input3_test)\n",
    "\n",
    "    #####3rd NN\n",
    "    m_error=len(X_error_train2[0,:])\n",
    "    p_error=np.size(y_error_train2[0])\n",
    "    N_error=len(X_error_train2)\n",
    "\n",
    "    error_model2= Sequential ([\n",
    "        Dense(hidden3, input_dim=m_error, activation='relu'), \n",
    "        Dropout(0.1),\n",
    "        Dense(second_layer3), #,activation='relu'),\n",
    "        Dropout(0.1),\n",
    "        Dense(third_layer3), #,activation='relu'),\n",
    "        Dropout(0.1),\n",
    "        Dense(forth_layer3), #,activation='relu'),\n",
    "        Dropout(0.1),\n",
    "        Dense(p_error)\n",
    "    ])\n",
    "\n",
    "    #error_model2.summary()\n",
    "\n",
    "    #sgd=keras.optimizers.legacy.SGD(learning_rate=0.05, momentum=0.75, decay=0.0, nesterov=False)\n",
    "    rmsprop = keras.optimizers.legacy.RMSprop(lr=0.05,momentum=0.75, decay=0.0)\n",
    "    error_model2.compile(loss='mean_squared_error', optimizer=adam, metrics=['mse','mae','accuracy'])\n",
    "    history4=error_model2.fit(X_error_train2, y_error_train2, batch_size=N_error, epochs=300, shuffle=False, verbose=0)\n",
    "\n",
    "\n",
    "    error_predicted_tr2 = error_model2.predict(X_error_train2)\n",
    "    error_predicted_tr2 = np.reshape(error_predicted_tr2, (error_predicted_tr2.size,))\n",
    "    error_predicted_tes2 = error_model2.predict( X_error_test2)\n",
    "    error_predicted_tes2= np.reshape(error_predicted_tes2, (error_predicted_tes2.size,))\n",
    "\n",
    "    compensated_y_train=compensated1_train-(error_predicted_tr2+subtraction_error_train2)\n",
    "    compensated_y_test=compensated1_test-(error_predicted_tes2+subtraction_error_test2)\n",
    "\n",
    "    error_predicted_tr3=error_predicted_tr2+subtraction_error_train2\n",
    "    error_predicted_tes3=error_predicted_tes2+subtraction_error_test2\n",
    "\n",
    "    training_final_add=np.column_stack((predicted_train, error_predicted_tr))\n",
    "    training_final_add=np.column_stack((training_final_add,error_predicted_tr3))\n",
    "\n",
    "    test_final_add=np.column_stack((predicted_test, error_predicted_tes))\n",
    "    test_final_add=np.column_stack((test_final_add,error_predicted_tes3))\n",
    "\n",
    "    ####final NN\n",
    "    m_final=len(training_final_add[0,:])\n",
    "    p_final=np.size(y_train[0])\n",
    "    N_final=len(training_final_add)\n",
    "\n",
    "    final_model= Sequential ([\n",
    "        Dense(hidden4, input_dim=m_final, activation='relu'), \n",
    "    #    Dropout(0.1),\n",
    "    #    Dense(second_layer4), #,activation='relu'),\n",
    "    #    Dropout(0.1),\n",
    "    #    Dense(third_layer4), #,activation='relu'),\n",
    "    #    Dropout(0.1),\n",
    "    #    Dense(forth_layer4), #,activation='relu'),\n",
    "    #    Dropout(0.1),\n",
    "        Dense(p_final)\n",
    "    ])\n",
    "\n",
    "    #final_model.summary()\n",
    "\n",
    "    #sgd=keras.optimizers.legacy.SGD(learning_rate=0.05, momentum=0.75, decay=0.0, nesterov=False)\n",
    "    rmsprop = keras.optimizers.legacy.RMSprop(lr=0.05,momentum=0.75, decay=0.0)\n",
    "    final_model.compile(loss='mean_squared_error', optimizer=adam, metrics=['mse','mae','accuracy'])\n",
    "    final_history=final_model.fit(training_final_add, y_train, batch_size=N_final, epochs=300, shuffle=False, verbose=0)\n",
    "\n",
    "    final_predicted_tr = final_model.predict(training_final_add)\n",
    "    final_predicted_tr = np.reshape(final_predicted_tr, (final_predicted_tr.size,))\n",
    "    final_predicted_tes = final_model.predict(test_final_add)\n",
    "    final_predicted_tes = np.reshape(final_predicted_tes, (final_predicted_tes.size,))\n",
    "\n",
    "    y_train=y_train+subtraction_second_train\n",
    "    final_y_train=final_predicted_tr+subtraction_second_train\n",
    "    final_y_train = np.reshape(final_y_train, (final_y_train.size,))\n",
    "\n",
    "    final_error_train=final_y_train-y_train\n",
    "    final_rmse_error_train=np.sqrt(sum(final_error_train*final_error_train)/len(final_error_train))\n",
    "    final_mse_train=(sum(final_error_train*final_error_train)/len(final_error_train))\n",
    "    final_mape_train=100*sum(abs(final_error_train/y_train))/len(y_train)\n",
    "    final_mae_train=sum(abs(final_error_train-y_train))/len(y_train)\n",
    "    final_rmspe_train=100*np.sqrt(np.nanmean(np.square(((y_train - final_y_train) / y_train))))\n",
    "\n",
    "    \n",
    "    y_test=y_test+subtraction_second_test\n",
    "    \n",
    "    final_y_test=final_predicted_tes+subtraction_second_test\n",
    "    y_test = np.reshape(y_test, (y_test.size,))\n",
    "    final_y_test = np.reshape(final_y_test, (final_y_test.size,))\n",
    "\n",
    "\n",
    "    #final_error_test=y_test[:-1]-final_predicted_tes[:-1]\n",
    "    final_error_test=final_y_test[:-1]-y_test[:-1] \n",
    "    final_rmse_error_test=np.sqrt(sum(final_error_test*final_error_test)/len(final_error_test))\n",
    "    final_mse_test=(sum(final_error_test*final_error_test)/len(final_error_test))\n",
    "    final_mape_test=100*sum(abs(final_error_test/y_test[:-1]))/len(y_test-1)\n",
    "    final_mae_test=sum(abs(final_error_test-y_test[:-1]))/len(y_test-1)\n",
    "    final_rmspe_test=100*np.sqrt(np.nanmean(np.square(((y_test[:-1] - final_y_test[:-1]) / y_test[:-1]))))\n",
    "\n",
    "    #errors of the first nn\n",
    "    predicted_train=predicted_train+subtraction_second_train\n",
    "    predicted_test=predicted_test+subtraction_second_test\n",
    "\n",
    "    predicted_error_train=predicted_train-y_train\n",
    "    predicted_rmse_error_train=np.sqrt(sum(predicted_error_train*predicted_error_train)/len(predicted_error_train))\n",
    "    predicted_mse_train=(sum(predicted_error_train*predicted_error_train)/len(predicted_error_train))\n",
    "    predicted_mape_train=100*sum(abs(predicted_error_train/y_train))/len(y_train)\n",
    "    predicted_mae_train=sum(abs(predicted_error_train-y_train))/len(y_train)\n",
    "    predicted_rmspe_train=100*np.sqrt(np.nanmean(np.square(((y_train - predicted_train) / y_train))))\n",
    "\n",
    "    predicted_error_test=predicted_test[:-1]-y_test[:-1]\n",
    "    predicted_rmse_error_test=np.sqrt(sum(predicted_error_test*predicted_error_test)/len(predicted_error_test))\n",
    "    predicted_mse_test=(sum(predicted_error_test*predicted_error_test)/len(predicted_error_test))\n",
    "    predicted_mape_test=100*sum(abs(predicted_error_test/y_test[:-1]))/len(y_test-1)\n",
    "    predicted_mae_test=sum(abs(predicted_error_test-y_test[:-1]))/len(y_test-1)\n",
    "    predicted_rmspe_test=100*np.sqrt(np.nanmean(np.square(((y_test[:-1] - predicted_test[:-1]) / y_test[:-1]))))\n",
    "\n",
    "    #errors of the second nn\n",
    "    compensated1_train_error=compensated1_train-y_train\n",
    "\n",
    "    compensated1_train_rmse_error_train=np.sqrt(sum(compensated1_train_error*compensated1_train_error)/len(compensated1_train_error))\n",
    "    compensated1_train_mse_train=(sum(compensated1_train_error*compensated1_train_error)/len(compensated1_train_error))\n",
    "    compensated1_train_mape_train=100*sum(abs(compensated1_train_error/y_train))/len(y_train)\n",
    "    compensated1_train_mae_train=sum(abs(compensated1_train_error-y_train))/len(y_train)\n",
    "    compensated1_train_rmspe_train=np.sqrt(np.nanmean(np.square(((y_train - compensated1_train) / y_train))))*100\n",
    "\n",
    "    compensated1_test_error=compensated1_test[:-1]-y_test[:-1]\n",
    "\n",
    "    compensated1_test_rmse_error_test=np.sqrt(sum(compensated1_test_error*compensated1_test_error)/len(compensated1_test_error))\n",
    "    compensated1_test_mse_test=(sum(compensated1_test_error*compensated1_test_error)/len(compensated1_test_error))\n",
    "    compensated1_test_mape_test=100*sum(abs(compensated1_test_error/y_test[:-1]))/len(y_test-1)\n",
    "    compensated1_test_mae_test=sum(abs(compensated1_test_error-y_test[:-1]))/len(y_test-1)\n",
    "    compensated1_test_rmspe_test=np.sqrt(np.nanmean(np.square(((y_test[:-1] - compensated1_test[:-1]) / y_test[:-1]))))*100\n",
    "\n",
    "    #errors of the third nn\n",
    "    compensated_error_train=compensated_y_train-y_train\n",
    "\n",
    "    comp_rmse_error_train=np.sqrt(sum(compensated_error_train*compensated_error_train)/len(compensated_error_train))\n",
    "    comp_mse_train=(sum(compensated_error_train*compensated_error_train)/len(compensated_error_train))\n",
    "    comp_mape_train=100*sum(abs(compensated_error_train/y_train))/len(y_train)\n",
    "    comp_mae_train=sum(abs(compensated_error_train-y_train))/len(y_train)\n",
    "    comp_rmspe_train=np.sqrt(np.nanmean(np.square(((y_train - compensated_y_train) / y_train))))*100\n",
    "\n",
    "    compensated_error_test=compensated_y_test[:-1]-y_test[:-1]\n",
    "\n",
    "    comp_rmse_error_test=np.sqrt(sum(compensated_error_test*compensated_error_test)/len(compensated_error_test))\n",
    "    comp_mse_test=(sum(compensated_error_test*compensated_error_test)/len(compensated_error_test))\n",
    "    comp_mape_test=100*sum(abs(compensated_error_test/y_test[:-1]))/len(y_test-1)\n",
    "    comp_mae_test=sum(abs(compensated_error_test-y_test[:-1]))/len(y_test-1)\n",
    "    comp_rmspe_test=np.sqrt(np.nanmean(np.square(((y_test[:-1] - compensated_y_test[:-1]) / y_test[:-1]))))*100\n",
    "\n",
    "    zz_rmse_errors_ttrain=(predicted_rmse_error_train,compensated1_train_rmse_error_train, comp_rmse_error_train,final_rmse_error_train)\n",
    "    zz_rmse_errors_test=(predicted_rmse_error_test,compensated1_test_rmse_error_test, comp_rmse_error_test,final_rmse_error_test)\n",
    "\n",
    "    zz_rmspe_errors_ttrain=(predicted_rmspe_train,compensated1_train_rmspe_train, comp_rmspe_train,final_rmspe_train)\n",
    "    zz_rmspe_errors_test=(predicted_rmspe_test,compensated1_test_rmspe_test, comp_rmspe_test,final_rmspe_test)\n",
    "\n",
    "    zz_mape_errors_ttrain=(predicted_mape_train,compensated1_train_mape_train, comp_mape_train,final_mape_train)\n",
    "    zz_mape_errors_test=(predicted_mape_test,compensated1_test_mape_test, comp_mape_test,final_mape_test)\n",
    "\n",
    "    zz_mae_errors_ttrain=(predicted_mae_train,compensated1_train_mae_train, comp_mae_train,final_mae_train)\n",
    "    zz_mae_errors_test=(predicted_mae_test,compensated1_test_mae_test, comp_mae_test,final_mae_test)\n",
    "\n",
    "    zz_predictions_train = (y_train, predicted_train,compensated1_train,  compensated_y_train, final_y_train)\n",
    "    zz_predictions_test = (y_test,predicted_test,compensated1_test, compensated_y_test, final_y_test)\n",
    "\n",
    "    return final_predicted_tes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9a0257df",
   "metadata": {},
   "source": [
    "## Finding predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e2bbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WOUT DWT\n",
    "\n",
    "preds = []\n",
    "for i in range(INPUT_SIZE):\n",
    "    preds.append(PECNET(final_first_w_input_train, output_w_train[:,i], final_first_w_input_test, output_w_test[:,i]))\n",
    "preds = np.array(preds)\n",
    "preds_last = []\n",
    "for i in range(len(output_test)):\n",
    "    preds_last.append(np.array(preds[:,i]))\n",
    "preds_last = np.array(preds_last)\n",
    "\n",
    "predicted_data_pecnet_wout_dwt = preds_last + subtraction_output_test[:, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b563c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WITH DWT\n",
    "\n",
    "preds = []\n",
    "for i in range(INPUT_SIZE-1):\n",
    "    preds.append(PECNET(final_first_w_input_train, output_w_train[:,i], final_first_w_input_test, output_w_test[:,i]))\n",
    "preds = np.array(preds)\n",
    "preds_w = []\n",
    "for i in range(len(output_test)):\n",
    "    w_coeffs = []\n",
    "    w_coeffs.append(0)\n",
    "    for elm in preds[:,i]:\n",
    "        w_coeffs.append(elm)\n",
    "    preds_w.append(np.array(w_coeffs))\n",
    "preds_w = np.array(preds_w)\n",
    "\n",
    "predicted_data_pecnet_dwt = idwt(preds_w) + subtraction_output_test[:, None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b394d202",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ((ax0,ax1),(ax2,ax3)) = plt.subplots(2,2, figsize=(10,10))\n",
    "plt.suptitle('Temperature Prediction')\n",
    "ax0.plot(history1.history[\"loss\"])\n",
    "ax0.set_title(\"First Network (X=Average)\")\n",
    "ax0.set_xlabel(\"Epoch\")\n",
    "ax0.set_ylabel(\"Loss\")\n",
    "\n",
    "ax1.plot(history3.history[\"loss\"])\n",
    "ax1.set_title(\"Second Network (X=Raw)\")\n",
    "ax1.set_xlabel(\"Epoch\")\n",
    "ax1.set_ylabel(\"Loss\")\n",
    "\n",
    "ax2.plot(history4.history[\"loss\"])\n",
    "ax2.set_title(\"Error Network\")\n",
    "ax2.set_xlabel(\"Epoch\")\n",
    "ax2.set_ylabel(\"Loss\")\n",
    "\n",
    "ax3.plot(final_history.history[\"loss\"])\n",
    "ax3.set_title(\"Final Network\")\n",
    "ax3.set_xlabel(\"Epoch\")\n",
    "ax3.set_ylabel(\"Loss\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7bc80eea",
   "metadata": {},
   "source": [
    "# errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f18f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train=y_train+subtraction_second_train\n",
    "final_y_train=final_predicted_tr+subtraction_second_train\n",
    "final_y_train = np.reshape(final_y_train, (final_y_train.size,))\n",
    "\n",
    "final_error_train=final_y_train-y_train\n",
    "final_rmse_error_train=np.sqrt(sum(final_error_train*final_error_train)/len(final_error_train))\n",
    "final_mse_train=(sum(final_error_train*final_error_train)/len(final_error_train))\n",
    "final_mape_train=100*sum(abs(final_error_train/y_train))/len(y_train)\n",
    "final_mae_train=sum(abs(final_error_train-y_train))/len(y_train)\n",
    "final_rmspe_train=100*np.sqrt(np.nanmean(np.square(((y_train - final_y_train) / y_train))))\n",
    "\n",
    " \n",
    "y_test=y_test+subtraction_second_test\n",
    " \n",
    "final_y_test=final_predicted_tes+subtraction_second_test\n",
    "y_test = np.reshape(y_test, (y_test.size,))\n",
    "final_y_test = np.reshape(final_y_test, (final_y_test.size,))\n",
    "\n",
    "\n",
    "#final_error_test=y_test[:-1]-final_predicted_tes[:-1]\n",
    "final_error_test=final_y_test[:-1]-y_test[:-1] \n",
    "final_rmse_error_test=np.sqrt(sum(final_error_test*final_error_test)/len(final_error_test))\n",
    "final_mse_test=(sum(final_error_test*final_error_test)/len(final_error_test))\n",
    "final_mape_test=100*sum(abs(final_error_test/y_test[:-1]))/len(y_test-1)\n",
    "final_mae_test=sum(abs(final_error_test-y_test[:-1]))/len(y_test-1)\n",
    "final_rmspe_test=100*np.sqrt(np.nanmean(np.square(((y_test[:-1] - final_y_test[:-1]) / y_test[:-1]))))\n",
    "\n",
    "#errors of the first nn\n",
    "predicted_train=predicted_train+subtraction_second_train\n",
    "predicted_test=predicted_test+subtraction_second_test\n",
    "\n",
    "predicted_error_train=predicted_train-y_train\n",
    "predicted_rmse_error_train=np.sqrt(sum(predicted_error_train*predicted_error_train)/len(predicted_error_train))\n",
    "predicted_mse_train=(sum(predicted_error_train*predicted_error_train)/len(predicted_error_train))\n",
    "predicted_mape_train=100*sum(abs(predicted_error_train/y_train))/len(y_train)\n",
    "predicted_mae_train=sum(abs(predicted_error_train-y_train))/len(y_train)\n",
    "predicted_rmspe_train=100*np.sqrt(np.nanmean(np.square(((y_train - predicted_train) / y_train))))\n",
    "\n",
    "predicted_error_test=predicted_test[:-1]-y_test[:-1]\n",
    "predicted_rmse_error_test=np.sqrt(sum(predicted_error_test*predicted_error_test)/len(predicted_error_test))\n",
    "predicted_mse_test=(sum(predicted_error_test*predicted_error_test)/len(predicted_error_test))\n",
    "predicted_mape_test=100*sum(abs(predicted_error_test/y_test[:-1]))/len(y_test-1)\n",
    "predicted_mae_test=sum(abs(predicted_error_test-y_test[:-1]))/len(y_test-1)\n",
    "predicted_rmspe_test=100*np.sqrt(np.nanmean(np.square(((y_test[:-1] - predicted_test[:-1]) / y_test[:-1]))))\n",
    "\n",
    "#errors of the second nn\n",
    "compensated1_train_error=compensated1_train-y_train\n",
    "\n",
    "compensated1_train_rmse_error_train=np.sqrt(sum(compensated1_train_error*compensated1_train_error)/len(compensated1_train_error))\n",
    "compensated1_train_mse_train=(sum(compensated1_train_error*compensated1_train_error)/len(compensated1_train_error))\n",
    "compensated1_train_mape_train=100*sum(abs(compensated1_train_error/y_train))/len(y_train)\n",
    "compensated1_train_mae_train=sum(abs(compensated1_train_error-y_train))/len(y_train)\n",
    "compensated1_train_rmspe_train=np.sqrt(np.nanmean(np.square(((y_train - compensated1_train) / y_train))))*100\n",
    "\n",
    "compensated1_test_error=compensated1_test[:-1]-y_test[:-1]\n",
    "\n",
    "compensated1_test_rmse_error_test=np.sqrt(sum(compensated1_test_error*compensated1_test_error)/len(compensated1_test_error))\n",
    "compensated1_test_mse_test=(sum(compensated1_test_error*compensated1_test_error)/len(compensated1_test_error))\n",
    "compensated1_test_mape_test=100*sum(abs(compensated1_test_error/y_test[:-1]))/len(y_test-1)\n",
    "compensated1_test_mae_test=sum(abs(compensated1_test_error-y_test[:-1]))/len(y_test-1)\n",
    "compensated1_test_rmspe_test=np.sqrt(np.nanmean(np.square(((y_test[:-1] - compensated1_test[:-1]) / y_test[:-1]))))*100\n",
    "\n",
    "#errors of the third nn\n",
    "compensated_error_train=compensated_y_train-y_train\n",
    "\n",
    "comp_rmse_error_train=np.sqrt(sum(compensated_error_train*compensated_error_train)/len(compensated_error_train))\n",
    "comp_mse_train=(sum(compensated_error_train*compensated_error_train)/len(compensated_error_train))\n",
    "comp_mape_train=100*sum(abs(compensated_error_train/y_train))/len(y_train)\n",
    "comp_mae_train=sum(abs(compensated_error_train-y_train))/len(y_train)\n",
    "comp_rmspe_train=np.sqrt(np.nanmean(np.square(((y_train - compensated_y_train) / y_train))))*100\n",
    "\n",
    "compensated_error_test=compensated_y_test[:-1]-y_test[:-1]\n",
    "\n",
    "comp_rmse_error_test=np.sqrt(sum(compensated_error_test*compensated_error_test)/len(compensated_error_test))\n",
    "comp_mse_test=(sum(compensated_error_test*compensated_error_test)/len(compensated_error_test))\n",
    "comp_mape_test=100*sum(abs(compensated_error_test/y_test[:-1]))/len(y_test-1)\n",
    "comp_mae_test=sum(abs(compensated_error_test-y_test[:-1]))/len(y_test-1)\n",
    "comp_rmspe_test=np.sqrt(np.nanmean(np.square(((y_test[:-1] - compensated_y_test[:-1]) / y_test[:-1]))))*100\n",
    "\n",
    "\n",
    "zz_rmse_errors_ttrain=(predicted_rmse_error_train,compensated1_train_rmse_error_train, comp_rmse_error_train,final_rmse_error_train)\n",
    "zz_rmse_errors_test=(predicted_rmse_error_test,compensated1_test_rmse_error_test, comp_rmse_error_test,final_rmse_error_test)\n",
    "\n",
    "zz_rmspe_errors_ttrain=(predicted_rmspe_train,compensated1_train_rmspe_train, comp_rmspe_train,final_rmspe_train)\n",
    "zz_rmspe_errors_test=(predicted_rmspe_test,compensated1_test_rmspe_test, comp_rmspe_test,final_rmspe_test)\n",
    "\n",
    "zz_mape_errors_ttrain=(predicted_mape_train,compensated1_train_mape_train, comp_mape_train,final_mape_train)\n",
    "zz_mape_errors_test=(predicted_mape_test,compensated1_test_mape_test, comp_mape_test,final_mape_test)\n",
    "\n",
    "zz_mae_errors_ttrain=(predicted_mae_train,compensated1_train_mae_train, comp_mae_train,final_mae_train)\n",
    "zz_mae_errors_test=(predicted_mae_test,compensated1_test_mae_test, comp_mae_test,final_mae_test)\n",
    "\n",
    "zz_predictions_train = (y_train, predicted_train,compensated1_train,  compensated_y_train, final_y_train)\n",
    "zz_predictions_test = (y_test,predicted_test,compensated1_test, compensated_y_test, final_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addbdbc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"PECNET with wavelet MAPE:\", mean_squared_error(get_data('original_code/data/hourly/63.11.HG.csv', type=13, start=12077, stop=13346), get_data('original_code/data/hourly/63.12.HG.csv', type=13, start=12065, stop=13334)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e43808a5",
   "metadata": {},
   "source": [
    "## Other Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1c93ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ONE SHIFTED PREDICTION ###################################################################################\n",
    "\n",
    "y_shifted = np.array(output_test[1:])\n",
    "print(\"One Shifted Input RMSE:\", mean_squared_error(output_test[:-1], y_shifted, squared=False))\n",
    "actual_data = [data for data in np.array(output_test)[:,0]][:-1][-100:]\n",
    "predicted_data = [data for data in y_shifted[:,0]][-100:]\n",
    "\n",
    "plt.plot(range(len(actual_data)), actual_data, label='Actual (Test)')\n",
    "plt.plot(range(len(actual_data)), predicted_data, label='Predicted (Test)')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel(data_type)\n",
    "plt.title('One Shifted Input (Window Size %d)' % INPUT_SIZE)\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "### STATISTICAL PREDICTION ###################################################################################\n",
    "\n",
    "stat_preds = []\n",
    "actual_values = []\n",
    "for i in range(len(output_test)):\n",
    "    data = output_test\n",
    "    if i-24-INPUT_SIZE < 0:\n",
    "        data = output_train\n",
    "    yesterday = data[i-24-INPUT_SIZE]\n",
    "    data = output_test\n",
    "    if i-24 < 0:\n",
    "        data = output_train\n",
    "    next_yesterday = data[i-24]\n",
    "    data = output_test\n",
    "    if i-INPUT_SIZE < 0:\n",
    "        data = output_train\n",
    "    today = data[i-INPUT_SIZE]\n",
    "    data = output_test\n",
    "    mean_yesterday = np.mean(yesterday)\n",
    "    std_yesterday = np.std(yesterday)\n",
    "    mean_today = np.mean(today)\n",
    "    std_today = np.std(today)\n",
    "\n",
    "    mean_proportion = mean_today/mean_yesterday\n",
    "    std_proportion = std_today/std_yesterday\n",
    "    pred = [data * mean_proportion for data in next_yesterday]\n",
    "    stat_preds.append(pred)\n",
    "\n",
    "    actual_values.append(output_test[i])\n",
    "\n",
    "print(\"Statistical MAPE:\", mean_absolute_percentage_error(stat_preds, actual_values))\n",
    "\n",
    "actual_data = [data for data in np.array(actual_values)[:,0]][-100:]\n",
    "predicted_data = [data for data in np.array(stat_preds)[:,0]][-100:]\n",
    "\n",
    "plt.plot(range(len(actual_data)), actual_data, label='Actual (Test)')\n",
    "plt.plot(range(len(actual_data)), predicted_data, label='Predicted (Test)')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel(data_type)\n",
    "plt.title('Statistical Approach (Window Size %d)' % INPUT_SIZE)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3ae717",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "### LINEAR REGRESSION ###################################################################################\n",
    "\n",
    "merged_successive_train = []\n",
    "for i in range(len(input_train)):\n",
    "    merged_successive_train.append(input_train[i] + input_train_today[i])\n",
    "\n",
    "merged_successive_test = []\n",
    "for i in range(len(input_test)):\n",
    "    merged_successive_test.append(input_test[i] + input_test_today[i])\n",
    "\n",
    "# Initialize linear regression model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Train the model\n",
    "model.fit(merged_successive_train, output_train)\n",
    "\n",
    "# Predict the next data point iteratively\n",
    "predicted_data_lr = model.predict(merged_successive_test)\n",
    "\n",
    "### MLP ###################################################################################\n",
    "\n",
    "# Initialize MLP regressor model\n",
    "model = MLPRegressor(hidden_layer_sizes=(100, 100), random_state=42)\n",
    "\n",
    "# Train the model\n",
    "model.fit(merged_successive_train, output_train)\n",
    "\n",
    "# Predict the next data point iteratively\n",
    "predicted_data_mlp = model.predict(merged_successive_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea92d31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "\n",
    "input_data=pd.read_csv('original_code/data/hourly/63.12.HG.csv')\n",
    "input_data=np.array(input_data)[12065:13334][:,5]\n",
    "# Example input data\n",
    "temperature_data = list(input_data)\n",
    "\n",
    "# Set the number of previous hours to use for prediction\n",
    "look_back = 2*INPUT_SIZE\n",
    "\n",
    "# Preprocess the data\n",
    "def create_dataset(data, look_back):\n",
    "    X, Y = [], []\n",
    "    for i in range(len(data) - look_back):\n",
    "        X.append(data[i:i + look_back])\n",
    "        Y.append(data[i + look_back])\n",
    "    return np.array(X), np.array(Y)\n",
    "\n",
    "X, Y = create_dataset(temperature_data, look_back)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "train_size = int(len(X) * 0.9)\n",
    "train_X, test_X = X[:train_size], X[train_size:]\n",
    "train_Y, test_Y = Y[:train_size], Y[train_size:]\n",
    "\n",
    "# Reshape the input data to fit the LSTM model requirements [samples, time steps, features]\n",
    "train_X = np.reshape(train_X, (train_X.shape[0], train_X.shape[1], 1))\n",
    "test_X = np.reshape(test_X, (test_X.shape[0], test_X.shape[1], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4bb97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, test_X = merged_successive_train, merged_successive_test\n",
    "train_Y, test_Y = output_train, output_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13c0c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "# Build the LSTM model\n",
    "model = Sequential()\n",
    "model.add(LSTM(50, input_shape=(2*INPUT_SIZE, 1)))\n",
    "model.add(Dense(INPUT_SIZE))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_X, train_Y, epochs=100, batch_size=1, verbose=0)\n",
    "\n",
    "# Evaluate the model\n",
    "train_loss = model.evaluate(train_X, train_Y, verbose=0)\n",
    "test_loss = model.evaluate(test_X, test_Y, verbose=0)\n",
    "print(f\"Train Loss: {train_loss:.4f}\")\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "\n",
    "# Make predictions\n",
    "train_predictions = model.predict(train_X)\n",
    "predicted_data_lstm = model.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a663933d",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_vals = dict()\n",
    "lr_vals = dict()\n",
    "mlp_vals = dict()\n",
    "lstm_vals = dict()\n",
    "pecnet_vals = dict()\n",
    "pecnet_dwt_vals = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e431ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Window Size: %d\" % INPUT_SIZE)\n",
    "print(\"Data Type: T+RH->WS\")\n",
    "print(\"Metric: RMSE\")\n",
    "print(\"LR MAPE:\", mean_squared_error(output_test, predicted_data_lr, squared=False))\n",
    "print(\"MLP MAPE:\", mean_squared_error(output_test, predicted_data_mlp, squared=False))\n",
    "print(\"LSTM MAPE:\", mean_squared_error(output_test, predicted_data_lstm, squared=False))\n",
    "print(\"PECNET without wavelet MAPE:\", mean_squared_error(output_test, predicted_data_pecnet_wout_dwt, squared=False))\n",
    "print(\"PECNET with wavelet MAPE:\", mean_squared_error(output_test, predicted_data_pecnet_dwt, squared=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4647d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Window Size: %d\" % INPUT_SIZE)\n",
    "print(\"Data Type: T+T->T\")\n",
    "print(\"Metric: MAPE\")\n",
    "print(\"LR MAPE:\", mean_absolute_percentage_error(output_test, predicted_data_lr))\n",
    "print(\"MLP MAPE:\", mean_absolute_percentage_error(output_test, predicted_data_mlp))\n",
    "print(\"LSTM MAPE:\", mean_absolute_percentage_error(output_test, predicted_data_lstm))\n",
    "print(\"PECNET without wavelet MAPE:\", mean_absolute_percentage_error(output_test, predicted_data_pecnet_wout_dwt))\n",
    "print(\"PECNET with wavelet MAPE:\", mean_absolute_percentage_error(output_test, predicted_data_pecnet_dwt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2d41c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = \"8T+RH->WS\"\n",
    "output_vals[key] = output_test\n",
    "lr_vals[key] = predicted_data_lr\n",
    "mlp_vals[key] = predicted_data_mlp\n",
    "lstm_vals[key] = predicted_data_lstm\n",
    "pecnet_vals[key] = predicted_data_pecnet_wout_dwt\n",
    "pecnet_dwt_vals[key] = predicted_data_pecnet_dwt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026b2843",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = \"4T+RH->WS\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a301db0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(output_vals[key][31:55])), [data[-1] for data in output_vals[key]][31:55], label='Actual')\n",
    "plt.plot(range(len(output_vals[key][31:55])), [data[-1] for data in lr_vals[key]][31:55], label='LR')\n",
    "plt.plot(range(len(output_vals[key][31:55])), [data[-1] for data in mlp_vals[key]][31:55], label='MLP')\n",
    "plt.plot(range(len(output_vals[key][31:55])), [data[-1] for data in lstm_vals[key]][31:55], label='LSTM')\n",
    "plt.plot(range(len(output_vals[key][31:55])), [data[-1] for data in pecnet_vals[key]][31:55], label='PECNET')\n",
    "plt.plot(range(len(output_vals[key][31:55])), [data[-1] for data in pecnet_dwt_vals[key]][31:55], label='PECNET + DWT')\n",
    "plt.xlabel('Time (hours)')\n",
    "\n",
    "plt.ylabel('Wind Speed (m/s)')\n",
    "#plt.ylabel('Temperature (°C)')\n",
    "#plt.ylabel('Relative Humidity (%)')\n",
    "\n",
    "plt.rc('font', size=15)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=8)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=8)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=15)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=15)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=10)    # legend fontsize\n",
    "plt.rc('figure', titlesize=15)  # fontsize of the figure title\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479cd5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = lr_vals.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a24fc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "for elm in temp:\n",
    "    temp[elm] = temp[elm].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1227f513",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_vals[key] = output_test\n",
    "lr_vals[key] = predicted_data_lr\n",
    "mlp_vals[key] = predicted_data_mlp\n",
    "lstm_vals[key] = predicted_data_lstm\n",
    "pecnet_vals[key] = predicted_data_pecnet_wout_dwt\n",
    "pecnet_dwt_vals[key] = predicted_data_pecnet_dwt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b337445",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Your dictionary\n",
    "my_dict = temp\n",
    "\n",
    "# File path where you want to save the dictionary data\n",
    "file_path = \"multi_station_lr_vals.json\"\n",
    "\n",
    "# Save the dictionary to the file\n",
    "with open(file_path, \"w\") as file:\n",
    "    json.dump(my_dict, file)\n",
    "\n",
    "print(\"Dictionary saved to file successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50d09d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Specify the path to your JSON file\n",
    "single_station_lr_vals = 'single_station_lr_vals.json'\n",
    "single_station_mlp_vals = 'single_station_mlp_vals.json'\n",
    "single_station_lstm_vals = 'single_station_lstm_vals.json'\n",
    "single_station_output_vals = 'single_station_output_vals.json'\n",
    "single_station_pecnet_dwt_vals = 'single_station_pecnet_dwt_vals.json'\n",
    "single_station_pecnet_vals = 'single_station_pecnet_vals.json'\n",
    "\n",
    "# Function to read JSON data from file and load into a dictionary\n",
    "def read_json_file(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        data = json.load(file)\n",
    "    return data\n",
    "\n",
    "# Call the function to read the JSON data into a dictionary\n",
    "single_station_lr_vals = read_json_file(single_station_lr_vals)\n",
    "single_station_mlp_vals = read_json_file(single_station_mlp_vals)\n",
    "single_station_lstm_vals = read_json_file(single_station_lstm_vals)\n",
    "single_station_output_vals = read_json_file(single_station_output_vals)\n",
    "single_station_pecnet_dwt_vals = read_json_file(single_station_pecnet_dwt_vals)\n",
    "single_station_pecnet_vals = read_json_file(single_station_pecnet_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b82ccdab",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = '8WS'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa113c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(single_station_output_vals[key][21:45])), [data[-1] for data in single_station_output_vals[key]][21:45], label='Actual')\n",
    "plt.plot(range(len(single_station_output_vals[key][21:45])), [data[-1] for data in single_station_lr_vals[key]][21:45], label='LR')\n",
    "plt.plot(range(len(single_station_output_vals[key][21:45])), [data[-1] for data in single_station_mlp_vals[key]][21:45], label='MLP')\n",
    "plt.plot(range(len(single_station_output_vals[key][21:45])), [data[-1] for data in single_station_lstm_vals[key]][21:45], label='LSTM')\n",
    "plt.plot(range(len(single_station_output_vals[key][21:45])), [data[-1] for data in single_station_pecnet_vals[key]][21:45], label='PECNET')\n",
    "plt.plot(range(len(single_station_output_vals[key][21:45])), [data[-1] for data in single_station_pecnet_dwt_vals[key]][21:45], label='PECNET + DWT')\n",
    "plt.xlabel('Time (hours)')\n",
    "plt.ylabel('Wind Speed (m/s)')\n",
    "#plt.ylabel('Temperature (°C)')\n",
    "#plt.ylabel('Relative Humidity (%)')\n",
    "\n",
    "plt.rc('font', size=15)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=8)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=8)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=15)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=15)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=10)    # legend fontsize\n",
    "plt.rc('figure', titlesize=15)  # fontsize of the figure title\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608c752d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Read the data from the JSON file\n",
    "file_path = 'tr-cities.json'  # Replace with the actual file path\n",
    "data = gpd.read_file(file_path)\n",
    "\n",
    "# Plot the map\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "data.plot(ax=ax, color='lightgrey', edgecolor='black')\n",
    "\n",
    "# Mark the specified location on the map\n",
    "target_location = gpd.GeoDataFrame({'geometry': gpd.points_from_xy([39.983722], [36.966])})\n",
    "target_location.plot(ax=ax, color='red', markersize=100)\n",
    "ax.axis('off')\n",
    "ax.legend()\n",
    "# Show the plot\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9631a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import folium\n",
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file\n",
    "data = pd.read_csv('station_list.csv')\n",
    "\n",
    "# Create a map centered around Turkey\n",
    "map_turkey = folium.Map(location=[39.9208, 32.8541], zoom_start=6, tiles='cartodbpositron')\n",
    "\n",
    "# Iterate over each row in the data and add a circle marker for each station\n",
    "for index, row in data.iterrows():\n",
    "    code = row['code']\n",
    "    name = row['name']\n",
    "    lat = row['lat']\n",
    "    lon = row['lon']\n",
    "    col = 'blue'\n",
    "    if code == '63.09' or code == '63.10' or code == '63.14':\n",
    "        continue\n",
    "    if code == '63.11':\n",
    "        col = 'green'\n",
    "    if code == '63.12':\n",
    "        col = 'red'\n",
    "    \n",
    "    # Add a circle marker to the map\n",
    "    folium.CircleMarker(location=[lat, lon], radius=3, tooltip=f\"{code}: {name}\", fill=True, color=col, fill_color='blue').add_to(map_turkey)\n",
    "\n",
    "# Save the map as an HTML file\n",
    "map_turkey.save('turkey_map.html')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5713df0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for_print = get_data('original_code/data/hourly/63.11.HG.csv', type=5, start=12077, stop=13346)\n",
    "\n",
    "dict_print = dict()\n",
    "for i in range(200):\n",
    "    if i < 125 or i >= 165:\n",
    "        dict_print[i] = for_print[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20c84bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for_print = get_data('original_code/data/hourly/63.12.HG.csv', type=13, start=12077, stop=13346)\n",
    "\n",
    "dict1 = dict()\n",
    "dict2 = dict()\n",
    "for i in range(200):\n",
    "    if i < 83:\n",
    "        dict1[i] = for_print[i]\n",
    "    if i > 115:\n",
    "        dict2[i] = for_print[i]\n",
    "\n",
    "list1 = sorted(dict1.items()) # sorted by key, return a list of tuples\n",
    "list2 = sorted(dict2.items()) # sorted by key, return a list of tuples\n",
    "x1, y1 = zip(*list1) # unpack a list of pairs into two tuples\n",
    "plt.plot(x1, y1, color='green')\n",
    "x2, y2 = zip(*list2) # unpack a list of pairs into two tuples\n",
    "plt.plot(x2, y2, color='green')\n",
    "\n",
    "#plt.plot(range(200), dict_print)\n",
    "plt.xlabel('Time (hours)')\n",
    "plt.ylabel('Wind Speed (m/s)')\n",
    "#plt.ylabel('Temperature (°C)')\n",
    "#plt.ylabel('Relative Humidity (%)')\n",
    "\n",
    "plt.rc('font', size=15)          # controls default text sizes\n",
    "plt.rc('axes', titlesize=8)     # fontsize of the axes title\n",
    "plt.rc('axes', labelsize=8)    # fontsize of the x and y labels\n",
    "plt.rc('xtick', labelsize=15)    # fontsize of the tick labels\n",
    "plt.rc('ytick', labelsize=15)    # fontsize of the tick labels\n",
    "plt.rc('legend', fontsize=10)    # legend fontsize\n",
    "plt.rc('figure', titlesize=15)  # fontsize of the figure title\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7d8951838d5117fcbb66acd20a3271b2de0c7f82d5e92a0ba87e59707f806a06"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('pecnet')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
